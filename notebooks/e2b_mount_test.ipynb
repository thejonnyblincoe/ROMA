{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2B Unified Mount Directory Test\n",
    "\n",
    "**‚úÖ E2B Integration with Unified Mount Structure**\n",
    "\n",
    "## üéØ Purpose\n",
    "Test the **universal mount directory structure** across all environments:\n",
    "- **Local**: Uses `S3_MOUNT_DIR` environment variable (default: `/opt/sentient`)\n",
    "- **Docker**: Uses `S3_MOUNT_DIR` environment variable (default: `/opt/sentient`)\n",
    "- **E2B**: Uses `S3_MOUNT_DIR` environment variable (default: `/opt/sentient`)\n",
    "\n",
    "## üìÅ Expected Structure\n",
    "```\n",
    "/opt/sentient/\n",
    "‚îú‚îÄ‚îÄ {project_id}/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ binance_toolkit/     # Binance API data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ coingecko_toolkit/   # CoinGecko API data  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ defillama_toolkit/   # DefiLlama API data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ arkham_toolkit/      # Arkham API data\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results/             # AI analysis outputs\n",
    "‚îî‚îÄ‚îÄ other_projects/\n",
    "```\n",
    "\n",
    "## üîÑ Test Flow\n",
    "1. üè† **Local Data Fetching**: Agent with data toolkit fetches crypto data to mount directory\n",
    "2. ‚òÅÔ∏è **S3 Sync**: Data automatically syncs to S3 bucket via mount\n",
    "3. üöÄ **E2B Execution**: Code runs in sandbox with same mount path `/opt/sentient`\n",
    "4. üßÆ **Analysis**: Sandbox analyzes data and saves results to mount\n",
    "5. üìä **Results**: Results visible across all environments\n",
    "\n",
    "## ‚úÖ What's Working\n",
    "- Universal `/opt/sentient` mount path across environments\n",
    "- S3 bucket mounted with goofys ‚úÖ **WORKING!**\n",
    "- Cross-environment data persistence ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Environment cleaned and ready\n"
     ]
    }
   ],
   "source": [
    "# Clean imports and setup\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Clean any cached modules\n",
    "modules_to_remove = [m for m in sys.modules.keys() if 'sentient' in m.lower()]\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "gc.collect()\n",
    "print(\"üîÑ Environment cleaned and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Dependencies and Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Configuration Check:\n",
      "==================================================\n",
      "‚úÖ E2B_API_KEY: e2b_d5d5...bd00\n",
      "‚úÖ S3_BUCKET_NAME: roma-shared\n",
      "‚úÖ S3_MOUNT_DIR: /opt/sentient\n",
      "\n",
      "üìã Optional Configuration:\n",
      "‚ÑπÔ∏è  E2B_TEMPLATE_ID: Using default - Custom E2B template (defaults to sentient-e2b-s3)\n",
      "‚ÑπÔ∏è  CURRENT_PROJECT_ID: Using default - Current project ID for testing\n",
      "\n",
      "üéâ All required environment variables are configured!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check required environment variables\n",
    "required_env_vars = {\n",
    "    'E2B_API_KEY': 'E2B API key for sandbox access',\n",
    "    'S3_BUCKET_NAME': 'S3 bucket name for data storage',\n",
    "    'S3_MOUNT_DIR': 'S3 mount directory path (default: /sentient)',\n",
    "}\n",
    "\n",
    "optional_env_vars = {\n",
    "    'E2B_TEMPLATE_ID': 'Custom E2B template (defaults to sentient-e2b-s3)',\n",
    "    'CURRENT_PROJECT_ID': 'Current project ID for testing',\n",
    "}\n",
    "\n",
    "print(\"üîç Environment Configuration Check:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check required variables\n",
    "missing_required = []\n",
    "for var, description in required_env_vars.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        if var in ['E2B_API_KEY']:\n",
    "            masked_value = f\"{value[:8]}...{value[-4:]}\"\n",
    "        else:\n",
    "            masked_value = value\n",
    "        print(f\"‚úÖ {var}: {masked_value}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {var}: MISSING - {description}\")\n",
    "        missing_required.append(var)\n",
    "\n",
    "print(\"\\nüìã Optional Configuration:\")\n",
    "for var, description in optional_env_vars.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        print(f\"‚úÖ {var}: {value}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  {var}: Using default - {description}\")\n",
    "\n",
    "\n",
    "mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "\n",
    "\n",
    "if missing_required:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing required environment variables: {', '.join(missing_required)}\")\n",
    "    print(\"   Please update your .env file with the missing values.\")\n",
    "else:\n",
    "    print(\"\\nüéâ All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ E2BTools imported successfully\n",
      "‚úÖ All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import E2B and agno dependencies\n",
    "try:\n",
    "    from agno.tools.e2b import E2BTools\n",
    "    print(\"‚úÖ E2BTools imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import E2BTools: {e}\")\n",
    "    print(\"   Please install agno: pip install agno-ai\")\n",
    "\n",
    "# Import other standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß E2B output decoder functions loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Union\n",
    "\n",
    "def decode_e2b_output(raw_result: Any) -> str:\n",
    "    \"\"\"\n",
    "    Decode AgnoAgent E2BTools output properly.\n",
    "    \n",
    "    The AgnoAgent E2BTools.run_python_code() method returns JSON-encoded output \n",
    "    in the format: [\"Logs:\\nLogs(stdout: [...], stderr: [...])\"]\n",
    "    This function properly decodes the Unicode escapes and extracts the stdout content.\n",
    "    \n",
    "    Args:\n",
    "        raw_result: Raw output from E2BTools.run_python_code()\n",
    "        \n",
    "    Returns:\n",
    "        Properly decoded and formatted output string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle different input types\n",
    "        if isinstance(raw_result, list) and len(raw_result) > 0:\n",
    "            content = raw_result[0]\n",
    "        elif isinstance(raw_result, str):\n",
    "            content = raw_result\n",
    "        else:\n",
    "            return str(raw_result)\n",
    "        \n",
    "        # Handle JSON-encoded list format: [\"Logs:\\nLogs(stdout: [...], stderr: [...])\"]\n",
    "        if isinstance(content, str) and content.startswith('[\"') and content.endswith('\"]'):\n",
    "            try:\n",
    "                parsed_list = json.loads(content)\n",
    "                if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "                    content = parsed_list[0]\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Extract stdout content from Logs format: \"Logs:\\nLogs(stdout: [...], stderr: [...])\"\n",
    "        if isinstance(content, str) and \"Logs(stdout:\" in content:\n",
    "            # Use regex to extract stdout content\n",
    "            stdout_pattern = r\"stdout:\\s*\\[(.*?)\\],?\\s*stderr:\"\n",
    "            match = re.search(stdout_pattern, content, re.DOTALL)\n",
    "            \n",
    "            if match:\n",
    "                stdout_raw = match.group(1)\n",
    "                \n",
    "                # Parse the stdout array content - handle quoted strings with proper escaping\n",
    "                stdout_lines = []\n",
    "                \n",
    "                # Split by ', ' but handle embedded quotes and escapes\n",
    "                current_part = \"\"\n",
    "                in_quotes = False\n",
    "                escaped = False\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(stdout_raw):\n",
    "                    char = stdout_raw[i]\n",
    "                    \n",
    "                    if escaped:\n",
    "                        current_part += char\n",
    "                        escaped = False\n",
    "                    elif char == '\\\\':\n",
    "                        current_part += char\n",
    "                        escaped = True\n",
    "                    elif char == \"'\" and not escaped:\n",
    "                        in_quotes = not in_quotes\n",
    "                        current_part += char\n",
    "                    elif char == ',' and not in_quotes and i + 1 < len(stdout_raw) and stdout_raw[i + 1] == ' ':\n",
    "                        # Found separator\n",
    "                        if current_part.strip():\n",
    "                            stdout_lines.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "                        i += 1  # Skip the space after comma\n",
    "                    else:\n",
    "                        current_part += char\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                # Add the last part\n",
    "                if current_part.strip():\n",
    "                    stdout_lines.append(current_part.strip())\n",
    "                \n",
    "                # Clean and decode each part\n",
    "                decoded_lines = []\n",
    "                for part in stdout_lines:\n",
    "                    # Remove outer quotes\n",
    "                    if (part.startswith(\"'\") and part.endswith(\"'\")) or (part.startswith('\"') and part.endswith('\"')):\n",
    "                        part = part[1:-1]\n",
    "                    \n",
    "                    # Decode JSON escapes properly\n",
    "                    try:\n",
    "                        # Handle JSON string encoding with proper Unicode support\n",
    "                        decoded_part = json.loads(f'\"{part}\"')\n",
    "                        decoded_lines.append(decoded_part)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Fallback: manual decode common escapes\n",
    "                        decoded_part = part\n",
    "                        decoded_part = decoded_part.replace('\\\\n', '\\n')\n",
    "                        decoded_part = decoded_part.replace('\\\\t', '\\t')\n",
    "                        decoded_part = decoded_part.replace('\\\\\"', '\"')\n",
    "                        decoded_part = decoded_part.replace(\"\\\\'\", \"'\")\n",
    "                        # Handle Unicode escapes manually\n",
    "                        decoded_part = decode_unicode_escapes(decoded_part)\n",
    "                        decoded_lines.append(decoded_part)\n",
    "                \n",
    "                # Join all stdout lines\n",
    "                full_output = ''.join(decoded_lines)\n",
    "                return full_output\n",
    "        \n",
    "        # If not in Logs format, try direct JSON decode\n",
    "        if isinstance(content, str):\n",
    "            try:\n",
    "                decoded = json.loads(f'\"{content}\"')\n",
    "                return decode_unicode_escapes(decoded)\n",
    "            except json.JSONDecodeError:\n",
    "                return decode_unicode_escapes(content)\n",
    "        \n",
    "        return str(content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"[Error decoding E2B output: {e}]\\n\\nRaw output:\\n{str(raw_result)}\"\n",
    "\n",
    "\n",
    "def decode_unicode_escapes(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Decode Unicode escape sequences in text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text that may contain Unicode escapes like \\\\ud83c\\\\udfd7\\\\ufe0f\n",
    "        \n",
    "    Returns:\n",
    "        Text with Unicode escapes properly decoded\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    try:\n",
    "        # Handle Unicode escapes using encode/decode\n",
    "        return text.encode().decode('unicode_escape')\n",
    "    except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "        # Fallback: manual decode using codecs\n",
    "        try:\n",
    "            import codecs\n",
    "            return codecs.decode(text, 'unicode_escape')\n",
    "        except:\n",
    "            # Final fallback: return as-is\n",
    "            return text\n",
    "\n",
    "print(\"üîß E2B output decoder functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è MOUNT DIRECTORY VERIFICATION TEST\n",
      "=============================================\n",
      "üéØ Template: sentient-e2b-s3\n",
      "üìÅ Mount directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "‚úÖ E2B toolkit initialized\n",
      "‚úÖ Mount verification completed\n",
      "üìä Output:\n",
      "----------------------------------------\n",
      "üèóÔ∏è E2B MOUNT VERIFICATION\n",
      "==============================\n",
      "Platform: Linux-6.1.102-x86_64-with-glibc2.41\n",
      "Python: 3.12.11\n",
      "Working dir: /home/user\n",
      "\n",
      "üìÅ Checking mount directory: /opt/sentient\n",
      "   (S3_MOUNT_DIR env var: /opt/sentient)\n",
      "‚úÖ Mount directory exists\n",
      "‚úÖ Mount directory readable: 2 items\n",
      "üìã Current contents:\n",
      "   üìÅ shared: 0 items\n",
      "   üìÅ {shared}: 0 items\n",
      "‚úÖ Mount directory is writable\n",
      "üßπ Test file cleaned up\n",
      "\n",
      "‚úÖ Mount verification completed\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "üéØ Mount test result: ‚úÖ PASSED\n",
      "\n",
      "üéâ SUCCESS: Mount directory is working correctly!\n",
      "üìÅ Using mount path: /opt/sentient\n"
     ]
    }
   ],
   "source": [
    "async def test_mount_directory():\n",
    "    \"\"\"Test the unified mount directory in E2B sandbox.\"\"\"\n",
    "    print(\"üèóÔ∏è MOUNT DIRECTORY VERIFICATION TEST\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        \n",
    "        print(f\"üéØ Template: {template_name}\")\n",
    "        print(f\"üìÅ Mount directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Initialize E2B\n",
    "        e2b_toolkit = E2BTools(\n",
    "            timeout=300,\n",
    "            sandbox_options={\"template\": template_name}\n",
    "        )\n",
    "        print(\"‚úÖ E2B toolkit initialized\")\n",
    "        \n",
    "        # Test mount directory and explore available directories\n",
    "        mount_test_code = f\"\"\"\n",
    "import os\n",
    "import platform\n",
    "\n",
    "print(\"üèóÔ∏è E2B MOUNT VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Platform: {{platform.platform()}}\")\n",
    "print(f\"Python: {{platform.python_version()}}\")\n",
    "print(f\"Working dir: {{os.getcwd()}}\")\n",
    "\n",
    "# Get mount directory from environment variable in E2B sandbox\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "print(f\"\\\\nüìÅ Checking mount directory: {{MOUNT_DIR}}\")\n",
    "print(f\"   (S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}})\")\n",
    "\n",
    "if os.path.exists(MOUNT_DIR):\n",
    "    print(\"‚úÖ Mount directory exists\")\n",
    "    \n",
    "    try:\n",
    "        contents = os.listdir(MOUNT_DIR)\n",
    "        print(f\"‚úÖ Mount directory readable: {{len(contents)}} items\")\n",
    "        \n",
    "        # Show contents\n",
    "        if contents:\n",
    "            print(\"üìã Current contents:\")\n",
    "            for item in contents[:10]:  # Show first 10 items\n",
    "                item_path = os.path.join(MOUNT_DIR, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    try:\n",
    "                        sub_items = len(os.listdir(item_path))\n",
    "                        print(f\"   üìÅ {{item}}: {{sub_items}} items\")\n",
    "                    except:\n",
    "                        print(f\"   üìÅ {{item}}: (cannot read)\")\n",
    "                else:\n",
    "                    size = os.path.getsize(item_path)\n",
    "                    print(f\"   üìÑ {{item}}: {{size}} bytes\")\n",
    "            \n",
    "            if len(contents) > 10:\n",
    "                print(f\"   ... and {{len(contents) - 10}} more items\")\n",
    "        else:\n",
    "            print(\"üìÅ Mount directory is empty\")\n",
    "            \n",
    "        # Test write access\n",
    "        test_file = os.path.join(MOUNT_DIR, \"mount_test.txt\")\n",
    "        try:\n",
    "            with open(test_file, 'w') as f:\n",
    "                f.write(\"Mount write test successful\")\n",
    "            \n",
    "            if os.path.exists(test_file):\n",
    "                print(\"‚úÖ Mount directory is writable\")\n",
    "                os.remove(test_file)  # Clean up\n",
    "                print(\"üßπ Test file cleaned up\")\n",
    "            else:\n",
    "                print(\"‚ùå Write test file not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Cannot write to mount directory: {{e}}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cannot read mount directory: {{e}}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Mount directory does not exist\")\n",
    "    print(f\"   Expected path: {{MOUNT_DIR}}\")\n",
    "    print(\"   This indicates the E2B template mount is not working properly\")\n",
    "    \n",
    "    # Explore available directories to help with debugging\n",
    "    print(\"\\\\nüîç EXPLORING AVAILABLE DIRECTORIES:\")\n",
    "    \n",
    "    # Check common mount points and symlinks from startup.sh\n",
    "    potential_mounts = [\n",
    "        \"/workspace\", \"/home/user\", \"/tmp\", \"/mnt\", \"/media\", \"/opt\",\n",
    "        \"/workspace/data\", \"/workspace/results\", \"/home/user/data\",\n",
    "        \"/home/user/s3-bucket\", \"/workspace/s3-bucket\"\n",
    "    ]\n",
    "    \n",
    "    found_dirs = []\n",
    "    for path in potential_mounts:\n",
    "        if os.path.exists(path):\n",
    "            found_dirs.append(path)\n",
    "            try:\n",
    "                contents = os.listdir(path)\n",
    "                print(f\"‚úÖ {{path}}: {{len(contents)}} items\")\n",
    "                \n",
    "                # Check if it's a symlink (from startup.sh)\n",
    "                if os.path.islink(path):\n",
    "                    target = os.readlink(path)\n",
    "                    print(f\"   üîó Symlink points to: {{target}}\")\n",
    "                \n",
    "                # Show contents\n",
    "                if contents and len(contents) <= 5:\n",
    "                    for item in contents:\n",
    "                        print(f\"   - {{item}}\")\n",
    "                elif contents:\n",
    "                    print(f\"   - {{contents[0]}}, {{contents[1]}} ... ({{len(contents)}} total)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úÖ {{path}}: exists but cannot list ({{e}})\")\n",
    "    \n",
    "    print(f\"\\\\nüìã Found {{len(found_dirs)}} potential directories\")\n",
    "    \n",
    "    # Check startup script environment info\n",
    "    print(\"\\\\nüìã E2B STARTUP ENVIRONMENT:\")\n",
    "    env_files = [\"/home/user/.env-info\", \"/workspace/.env-info\"]\n",
    "    for env_file in env_files:\n",
    "        if os.path.exists(env_file):\n",
    "            print(f\"Environment info from {{env_file}}:\")\n",
    "            try:\n",
    "                with open(env_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if not line.startswith('#') and '=' in line:\n",
    "                            print(f\"   {{line.strip()}}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"   Cannot read {{env_file}}: {{e}}\")\n",
    "    \n",
    "    # Check mount method used\n",
    "    mount_method_file = \"/tmp/mount-method\"\n",
    "    if os.path.exists(mount_method_file):\n",
    "        try:\n",
    "            with open(mount_method_file, 'r') as f:\n",
    "                method = f.read().strip()\n",
    "            print(f\"\\\\nüîß S3 Mount method used: {{method}}\")\n",
    "        except:\n",
    "            print(\"\\\\nüîß S3 Mount method: unknown\")\n",
    "    \n",
    "    print(\"\\\\nüí° DIAGNOSIS:\")\n",
    "    print(\"1. Check if S3_MOUNT_DIR env var is properly set in E2B template\")\n",
    "    print(\"2. Verify AWS credentials were passed as build arguments\")\n",
    "    print(\"3. Check startup.sh logs for mount failures\")\n",
    "    print(\"4. Alternative: Use working symlinked path if available\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Mount verification completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(mount_test_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"‚úÖ Mount verification completed\")\n",
    "        print(\"üìä Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if mount exists for return value\n",
    "        mount_exists = \"‚úÖ Mount directory exists\" in decoded_output\n",
    "        \n",
    "        # Extract working mount path if available\n",
    "        if not mount_exists:\n",
    "            lines = decoded_output.split('\\n')\n",
    "            for line in lines:\n",
    "                # Look for working symlinks that point to S3\n",
    "                if \"üîó Symlink points to:\" in line and (\"/data\" in line or \"s3\" in line.lower()):\n",
    "                    potential_mount = line.split(\"points to: \")[-1].strip()\n",
    "                    print(f\"\\nüí° FOUND POTENTIAL WORKING MOUNT: {potential_mount}\")\n",
    "                    globals()['discovered_mount_dir'] = potential_mount\n",
    "                    break\n",
    "        \n",
    "        return mount_exists\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mount directory test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "mount_success = await test_mount_directory()\n",
    "print(f\"\\nüéØ Mount test result: {'‚úÖ PASSED' if mount_success else '‚ùå FAILED'}\")\n",
    "\n",
    "if not mount_success:\n",
    "    print(\"\\nüîß TROUBLESHOOTING GUIDE:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"The S3 mount directory is not working properly.\")\n",
    "    print()\n",
    "    print(\"üìã Root cause: E2B template was built without AWS credentials\")\n",
    "    print(\"üîß Solution: Rebuild E2B template with proper build arguments:\")\n",
    "    print()\n",
    "    print(\"cd docker/e2b-sandbox\")\n",
    "    print(\"e2b template build \\\\\")\n",
    "    print(\"    --build-arg AWS_ACCESS_KEY_ID='your_key' \\\\\")\n",
    "    print(\"    --build-arg AWS_SECRET_ACCESS_KEY='your_secret' \\\\\")\n",
    "    print(\"    --build-arg S3_BUCKET_NAME='your_bucket' \\\\\")\n",
    "    print(\"    --build-arg S3_MOUNT_DIR='/opt/sentient' \\\\\")\n",
    "    print(\"    --name sentient-e2b-s3 --force\")\n",
    "    print()\n",
    "    print(\"Or run: ./setup.sh --e2b-only\")\n",
    "    \n",
    "    if 'discovered_mount_dir' in globals():\n",
    "        print(f\"\\nüí° TEMPORARY WORKAROUND:\")\n",
    "        print(f\"Update .env with: S3_MOUNT_DIR={globals()['discovered_mount_dir']}\")\n",
    "else:\n",
    "    print(\"\\nüéâ SUCCESS: Mount directory is working correctly!\")\n",
    "    print(f\"üìÅ Using mount path: {mount_dir}\")\n",
    "    globals()['working_mount_dir'] = mount_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Test 2: Project Structure Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ PROJECT STRUCTURE CREATION TEST\n",
      "=============================================\n",
      "üß™ Test Project ID: test_20250822_214954\n",
      "üìÅ Mount Directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "‚úÖ Using confirmed working mount: /opt/sentient\n",
      "‚úÖ Structure creation completed\n",
      "üìä Output:\n",
      "----------------------------------------\n",
      "üìÅ CREATING PROJECT STRUCTURE\n",
      "===================================\n",
      "Project: test_20250822_214954\n",
      "Mount: /opt/sentient\n",
      "S3_MOUNT_DIR env var: /opt/sentient\n",
      "‚úÖ Created project directory: /opt/sentient/test_20250822_214954\n",
      "\n",
      "üóÇÔ∏è  Creating 5 directories...\n",
      "‚úÖ binance_toolkit: created directory + test file (261 bytes)\n",
      "‚úÖ coingecko_toolkit: created directory + test file (263 bytes)\n",
      "‚úÖ defillama_toolkit: created directory + test file (263 bytes)\n",
      "‚úÖ arkham_toolkit: created directory + test file (260 bytes)\n",
      "‚úÖ results: created directory + test file (253 bytes)\n",
      "\n",
      "üìä Structure Creation Summary:\n",
      "   Final mount path used: /opt/sentient\n",
      "   Project directory: /opt/sentient/test_20250822_214954\n",
      "   Toolkit directories created: 5/5\n",
      "\n",
      "üìã Final structure verification:\n",
      "   üìÅ arkham_toolkit: 1 files\n",
      "      üìÑ structure_test.json (260 bytes)\n",
      "   üìÅ binance_toolkit: 1 files\n",
      "      üìÑ structure_test.json (261 bytes)\n",
      "   üìÅ coingecko_toolkit: 1 files\n",
      "      üìÑ structure_test.json (263 bytes)\n",
      "   üìÅ defillama_toolkit: 1 files\n",
      "      üìÑ structure_test.json (263 bytes)\n",
      "   üìÅ results: 1 files\n",
      "      üìÑ structure_test.json (253 bytes)\n",
      "\n",
      "üéâ Project structure created successfully!\n",
      "‚úÖ All expected directories and test files are present\n",
      "üîÑ This structure should be visible across all environments\n",
      "üåê S3 sync: Files should appear in S3 bucket via /opt/sentient mount\n",
      "\n",
      "‚úÖ Structure creation test completed\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "üéØ Structure test result: ‚úÖ PASSED\n"
     ]
    }
   ],
   "source": [
    "async def test_project_structure():\n",
    "    \"\"\"Test creating the expected project structure in mount directory.\"\"\"\n",
    "    print(\"üìÅ PROJECT STRUCTURE CREATION TEST\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "        \n",
    "        test_project_id = f\"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"üß™ Test Project ID: {test_project_id}\")\n",
    "        print(f\"üìÅ Mount Directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Use discovered working mount if available from previous test\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            working_mount = globals()['discovered_mount_dir']\n",
    "            print(f\"üîÑ Using discovered working mount: {working_mount}\")\n",
    "        elif 'working_mount_dir' in globals():\n",
    "            working_mount = globals()['working_mount_dir']\n",
    "            print(f\"‚úÖ Using confirmed working mount: {working_mount}\")\n",
    "        else:\n",
    "            working_mount = mount_dir\n",
    "            print(f\"üéØ Using configured mount: {working_mount}\")\n",
    "        \n",
    "        # Initialize E2B\n",
    "        e2b_toolkit = E2BTools(\n",
    "            timeout=300,\n",
    "            sandbox_options={\"template\": template_name}\n",
    "        )\n",
    "        \n",
    "        # Create project structure\n",
    "        structure_test_code = f\"\"\"\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Use environment variable or fallback to discovered mount\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "PROJECT_ID = \"{test_project_id}\"\n",
    "\n",
    "print(\"üìÅ CREATING PROJECT STRUCTURE\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Project: {{PROJECT_ID}}\")\n",
    "print(f\"Mount: {{MOUNT_DIR}}\")\n",
    "print(f\"S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}}\")\n",
    "\n",
    "# Check if mount directory is available (could be symlinked)\n",
    "if not os.path.exists(MOUNT_DIR):\n",
    "    print(f\"‚ùå Mount directory not available: {{MOUNT_DIR}}\")\n",
    "    \n",
    "    # Try alternative paths from startup.sh symlinks\n",
    "    alternatives = [\"/workspace/data\", \"/home/user/data\"]\n",
    "    for alt in alternatives:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"‚úÖ Using alternative mount: {{alt}}\")\n",
    "            MOUNT_DIR = alt\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚ùå No alternative mount directories found\")\n",
    "        exit(1)\n",
    "\n",
    "# Create project directory\n",
    "project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Created project directory: {{project_dir}}\")\n",
    "\n",
    "# Define expected toolkit directories\n",
    "toolkit_dirs = [\n",
    "    \"binance_toolkit\",\n",
    "    \"coingecko_toolkit\",\n",
    "    \"defillama_toolkit\", \n",
    "    \"arkham_toolkit\",\n",
    "    \"results\"\n",
    "]\n",
    "\n",
    "print(f\"\\\\nüóÇÔ∏è  Creating {{len(toolkit_dirs)}} directories...\")\n",
    "\n",
    "created_count = 0\n",
    "for toolkit_name in toolkit_dirs:\n",
    "    toolkit_dir = os.path.join(project_dir, toolkit_name)\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(toolkit_dir, exist_ok=True)\n",
    "        \n",
    "        # Create a test file in each directory\n",
    "        test_file = os.path.join(toolkit_dir, \"structure_test.json\")\n",
    "        test_data = {{\n",
    "            \"toolkit\": toolkit_name,\n",
    "            \"project_id\": PROJECT_ID,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"environment\": \"e2b_sandbox\",\n",
    "            \"mount_path\": MOUNT_DIR,\n",
    "            \"env_s3_mount_dir\": os.getenv('S3_MOUNT_DIR', 'not_set'),\n",
    "            \"test_type\": \"structure_creation\"\n",
    "        }}\n",
    "        \n",
    "        with open(test_file, 'w') as f:\n",
    "            json.dump(test_data, f, indent=2)\n",
    "        \n",
    "        file_size = os.path.getsize(test_file)\n",
    "        print(f\"‚úÖ {{toolkit_name}}: created directory + test file ({{file_size}} bytes)\")\n",
    "        created_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {{toolkit_name}}: failed to create - {{e}}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\\\nüìä Structure Creation Summary:\")\n",
    "print(f\"   Final mount path used: {{MOUNT_DIR}}\")\n",
    "print(f\"   Project directory: {{project_dir}}\")\n",
    "print(f\"   Toolkit directories created: {{created_count}}/{{len(toolkit_dirs)}}\")\n",
    "\n",
    "# Verify structure\n",
    "if os.path.exists(project_dir):\n",
    "    contents = os.listdir(project_dir)\n",
    "    print(f\"\\\\nüìã Final structure verification:\")\n",
    "    for item in sorted(contents):\n",
    "        item_path = os.path.join(project_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = os.listdir(item_path)\n",
    "            print(f\"   üìÅ {{item}}: {{len(files)}} files\")\n",
    "            for f in files:\n",
    "                f_path = os.path.join(item_path, f)\n",
    "                f_size = os.path.getsize(f_path)\n",
    "                print(f\"      üìÑ {{f}} ({{f_size}} bytes)\")\n",
    "\n",
    "if created_count == len(toolkit_dirs):\n",
    "    print(\"\\\\nüéâ Project structure created successfully!\")\n",
    "    print(\"‚úÖ All expected directories and test files are present\")\n",
    "    print(\"üîÑ This structure should be visible across all environments\")\n",
    "    print(f\"üåê S3 sync: Files should appear in S3 bucket via {{MOUNT_DIR}} mount\")\n",
    "else:\n",
    "    print(f\"\\\\n‚ö†Ô∏è  Partial success: {{created_count}}/{{len(toolkit_dirs)}} directories created\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Structure creation test completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(structure_test_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"‚úÖ Structure creation completed\")\n",
    "        print(\"üìä Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Save test project ID for later tests\n",
    "        globals()['test_project_id'] = test_project_id\n",
    "        \n",
    "        # Check if structure was created successfully\n",
    "        structure_success = \"üéâ Project structure created successfully!\" in decoded_output\n",
    "        \n",
    "        return structure_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Project structure test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "structure_success = await test_project_structure()\n",
    "print(f\"\\nüéØ Structure test result: {'‚úÖ PASSED' if structure_success else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Test 3: Data Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ DATA ANALYSIS WORKFLOW TEST\n",
      "========================================\n",
      "üîÑ Using existing project: test_20250822_012124\n",
      "üìÅ Mount Directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "‚úÖ Using confirmed working mount: /opt/sentient\n",
      "‚úÖ Analysis workflow completed\n",
      "üìä Output:\n",
      "----------------------------------------\n",
      "üí∞ DATA ANALYSIS WORKFLOW\n",
      "===================================\n",
      "Project: test_20250822_012124\n",
      "Mount: /opt/sentient\n",
      "S3_MOUNT_DIR env var: /opt/sentient)\n",
      "‚úÖ Project directory ready: /opt/sentient/test_20250822_012124\n",
      "\n",
      "üìä Step 1: Generating sample crypto data...\n",
      "‚úÖ Generated 120 data points for 5 symbols\n",
      "\n",
      "üîç Step 2: Performing market analysis...\n",
      "\n",
      "üí∞ Current Prices:\n",
      "   ADA: $0.49\n",
      "   BTC: $45,148.47\n",
      "   DOT: $24.36\n",
      "   ETH: $2,980.15\n",
      "   SOL: $100.54\n",
      "\n",
      "üìà 24h Price Changes:\n",
      "   üî¥ ADA: -3.69%\n",
      "   üî¥ BTC: -0.66%\n",
      "   üî¥ DOT: -6.06%\n",
      "   üî¥ ETH: -2.11%\n",
      "   üü¢ SOL: +0.98%\n",
      "\n",
      "üìä Price Volatility (24h):\n",
      "   ADA: 1.87%\n",
      "   BTC: 1.71%\n",
      "   DOT: 1.74%\n",
      "   ETH: 1.37%\n",
      "   SOL: 1.66%\n",
      "\n",
      "üíæ Step 3: Saving analysis results...\n",
      "‚úÖ Analysis results saved: /opt/sentient/test_20250822_012124/results/market_analysis.json (927 bytes)\n",
      "\"√¢¬ú¬Ö Individual symbol data saved to toolkit directories\n",
      "\n",
      "√∞¬ü¬ì¬ã Step 4: Verifying project structure...\n",
      "\n",
      "√∞¬ü¬ì¬Å Project 'test_20250822_012124' structure:\n",
      "   √∞¬ü¬ì¬Å ada_toolkit: 2 files1713 bytes total\n",
      "   üìÅ arkham_toolkit: 1 files260 bytes total\n",
      "   üìÅ binance_toolkit: 1 files261 bytes total\n",
      "   üìÅ btc_toolkit: 2 files1821 bytes total\n",
      "   üìÅ coingecko_toolkit: 1 files263 bytes total\n",
      "   üìÅ defillama_toolkit: 1 files263 bytes total\n",
      "   üìÅ dot_toolkit: 2 files1734 bytes total\n",
      "   üìÅ eth_toolkit: 2 files1793 bytes total\n",
      "   üìÅ results: 2 files1180 bytes total\n",
      "   üìÅ sol_toolkit: 2 files1750 bytes total\n",
      "\n",
      "√∞¬ü¬é¬â Data analysis workflow completed successfully!\n",
      "√¢¬ú¬Ö All data saved to unified mount directory\n",
      "√∞¬ü¬î¬Ñ Results should now be visible across all environments\n",
      "√∞¬ü¬å¬ê S3 sync: Files saved to /opt/sentient should sync to S3 bucket\n",
      "\n",
      "√¢¬ú¬Ö Analysis workflow test completed\n",
      "\"\n",
      "----------------------------------------\n",
      "\n",
      "üéØ Analysis workflow result: ‚ùå FAILED\n"
     ]
    }
   ],
   "source": [
    "async def test_data_analysis_workflow():\n",
    "    \"\"\"Test a complete data analysis workflow using the mount directory.\"\"\"\n",
    "    print(\"üí∞ DATA ANALYSIS WORKFLOW TEST\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "        mount_dir = os.path.expandvars(mount_dir)  # Resolve $HOME etc.\n",
    "        \n",
    "        # Use existing test project or create new one\n",
    "        if 'test_project_id' in globals():\n",
    "            project_id = globals()['test_project_id']\n",
    "            print(f\"üîÑ Using existing project: {project_id}\")\n",
    "        else:\n",
    "            project_id = f\"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            print(f\"üÜï Creating new project: {project_id}\")\n",
    "        \n",
    "        print(f\"üìÅ Mount Directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Use discovered working mount if available from previous test\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            working_mount = globals()['discovered_mount_dir']\n",
    "            print(f\"üîÑ Using discovered working mount: {working_mount}\")\n",
    "        elif 'working_mount_dir' in globals():\n",
    "            working_mount = globals()['working_mount_dir']\n",
    "            print(f\"‚úÖ Using confirmed working mount: {working_mount}\")\n",
    "        else:\n",
    "            working_mount = mount_dir\n",
    "            print(f\"üéØ Using configured mount: {working_mount}\")\n",
    "        \n",
    "        # Initialize E2B\n",
    "        e2b_toolkit = E2BTools(\n",
    "            timeout=300,\n",
    "            sandbox_options={\"template\": template_name}\n",
    "        )\n",
    "        \n",
    "        # Run data analysis workflow\n",
    "        analysis_code = f\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Use environment variable or fallback to discovered mount\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "PROJECT_ID = \"{project_id}\"\n",
    "\n",
    "print(\"üí∞ DATA ANALYSIS WORKFLOW\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Project: {{PROJECT_ID}}\")\n",
    "print(f\"Mount: {{MOUNT_DIR}}\")\n",
    "print(f\"S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}})\")\n",
    "\n",
    "# Check if mount directory is available (could be symlinked)\n",
    "if not os.path.exists(MOUNT_DIR):\n",
    "    print(f\"‚ùå Mount directory not available: {{MOUNT_DIR}}\")\n",
    "    \n",
    "    # Try alternative paths from startup.sh symlinks\n",
    "    alternatives = [\"/workspace/data\", \"/home/user/data\"]\n",
    "    for alt in alternatives:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"‚úÖ Using alternative mount: {{alt}}\")\n",
    "            MOUNT_DIR = alt\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚ùå No alternative mount directories found - using /tmp\")\n",
    "        MOUNT_DIR = \"/tmp\"\n",
    "\n",
    "# Ensure project structure exists\n",
    "project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = os.path.join(project_dir, \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Project directory ready: {{project_dir}}\")\n",
    "\n",
    "# Step 1: Generate sample crypto data (simulating toolkit data)\n",
    "print(\"\\\\nüìä Step 1: Generating sample crypto data...\")\n",
    "\n",
    "symbols = ['BTC', 'ETH', 'SOL', 'ADA', 'DOT']\n",
    "base_prices = {{\n",
    "    'BTC': 45000,\n",
    "    'ETH': 3000, \n",
    "    'SOL': 100,\n",
    "    'ADA': 0.50,\n",
    "    'DOT': 25\n",
    "}}\n",
    "\n",
    "# Generate 24 hours of hourly data\n",
    "hours = 24\n",
    "timestamps = [datetime.now() - timedelta(hours=i) for i in range(hours, 0, -1)]\n",
    "\n",
    "np.random.seed(42)  # Reproducible results\n",
    "crypto_data = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    base_price = base_prices[symbol]\n",
    "    \n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        # Simulate realistic price movement\n",
    "        volatility = np.random.normal(0, 0.02)  # 2% volatility\n",
    "        trend = 0.001 * np.sin(i / 6)  # 6-hour cycle\n",
    "        price = base_price * (1 + trend + volatility)\n",
    "        \n",
    "        crypto_data.append({{\n",
    "            'timestamp': timestamp.isoformat(),\n",
    "            'symbol': symbol,\n",
    "            'price': round(price, 6),\n",
    "            'volume': np.random.uniform(1000000, 5000000)\n",
    "        }})\n",
    "\n",
    "df = pd.DataFrame(crypto_data)\n",
    "print(f\"‚úÖ Generated {{len(df)}} data points for {{len(symbols)}} symbols\")\n",
    "\n",
    "# Step 2: Perform analysis\n",
    "print(\"\\\\nüîç Step 2: Performing market analysis...\")\n",
    "\n",
    "# Current prices\n",
    "current_prices = df.groupby('symbol')['price'].last()\n",
    "print(\"\\\\nüí∞ Current Prices:\")\n",
    "for symbol, price in current_prices.items():\n",
    "    print(f\"   {{symbol}}: ${{price:,.2f}}\")\n",
    "\n",
    "# Calculate 24h changes\n",
    "first_prices = df.groupby('symbol')['price'].first()\n",
    "changes_24h = ((current_prices - first_prices) / first_prices * 100)\n",
    "\n",
    "print(\"\\\\nüìà 24h Price Changes:\")\n",
    "for symbol, change in changes_24h.items():\n",
    "    emoji = \"üü¢\" if change > 0 else \"üî¥\" if change < 0 else \"‚ö™\"\n",
    "    print(f\"   {{emoji}} {{symbol}}: {{change:+.2f}}%\")\n",
    "\n",
    "# Calculate volatility\n",
    "volatility = df.groupby('symbol')['price'].std() / df.groupby('symbol')['price'].mean() * 100\n",
    "print(\"\\\\nüìä Price Volatility (24h):\")\n",
    "for symbol, vol in volatility.items():\n",
    "    print(f\"   {{symbol}}: {{vol:.2f}}%\")\n",
    "\n",
    "# Step 3: Save results to mount directory\n",
    "print(\"\\\\nüíæ Step 3: Saving analysis results...\")\n",
    "\n",
    "# Comprehensive analysis results\n",
    "analysis_results = {{\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'project_id': PROJECT_ID,\n",
    "    'analysis_type': 'crypto_market_analysis',\n",
    "    'data_period': '24h',\n",
    "    'symbols_analyzed': len(symbols),\n",
    "    'data_points': len(df),\n",
    "    'mount_path_used': MOUNT_DIR,\n",
    "    'env_s3_mount_dir': os.getenv('S3_MOUNT_DIR', 'not_set'),\n",
    "    'current_prices': dict(current_prices),\n",
    "    'price_changes_24h': dict(changes_24h),\n",
    "    'volatility_24h': dict(volatility),\n",
    "    'summary': {{\n",
    "        'best_performer': changes_24h.idxmax(),\n",
    "        'worst_performer': changes_24h.idxmin(),\n",
    "        'highest_volatility': volatility.idxmax(),\n",
    "        'lowest_volatility': volatility.idxmin()\n",
    "    }}\n",
    "}}\n",
    "\n",
    "# Save analysis results\n",
    "results_file = os.path.join(results_dir, 'market_analysis.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "results_size = os.path.getsize(results_file)\n",
    "print(f\"‚úÖ Analysis results saved: {{results_file}} ({{results_size}} bytes)\")\n",
    "\n",
    "# Save raw data for each toolkit\n",
    "for symbol in symbols:\n",
    "    symbol_data = df[df['symbol'] == symbol].copy()\n",
    "    toolkit_name = f\"{{symbol.lower()}}_toolkit\"  # Not exactly the real toolkit names, but for demo\n",
    "    toolkit_dir = os.path.join(project_dir, toolkit_name)\n",
    "    os.makedirs(toolkit_dir, exist_ok=True)\n",
    "    \n",
    "    # Save symbol-specific data\n",
    "    symbol_file = os.path.join(toolkit_dir, f'{{symbol.lower()}}_24h_data.csv')\n",
    "    symbol_data.to_csv(symbol_file, index=False)\n",
    "    \n",
    "    # Save symbol summary\n",
    "    summary_file = os.path.join(toolkit_dir, f'{{symbol.lower()}}_summary.json')\n",
    "    symbol_summary = {{\n",
    "        'symbol': symbol,\n",
    "        'current_price': float(current_prices[symbol]),\n",
    "        'change_24h': float(changes_24h[symbol]),\n",
    "        'volatility_24h': float(volatility[symbol]),\n",
    "        'data_points': len(symbol_data),\n",
    "        'mount_path_used': MOUNT_DIR,\n",
    "        'price_range': {{\n",
    "            'min': float(symbol_data['price'].min()),\n",
    "            'max': float(symbol_data['price'].max()),\n",
    "            'avg': float(symbol_data['price'].mean())\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(symbol_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Individual symbol data saved to toolkit directories\")\n",
    "\n",
    "# Step 4: Verify final structure\n",
    "print(\"\\\\nüìã Step 4: Verifying project structure...\")\n",
    "\n",
    "if os.path.exists(project_dir):\n",
    "    contents = os.listdir(project_dir)\n",
    "    print(f\"\\\\nüìÅ Project '{{PROJECT_ID}}' structure:\")\n",
    "    for item in sorted(contents):\n",
    "        item_path = os.path.join(project_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = os.listdir(item_path)\n",
    "            total_size = sum(os.path.getsize(os.path.join(item_path, f)) \n",
    "                           for f in files if os.path.isfile(os.path.join(item_path, f)))\n",
    "            print(f\"   üìÅ {{item}}: {{len(files)}} files, {{total_size}} bytes total\")\n",
    "\n",
    "print(\"\\\\nüéâ Data analysis workflow completed successfully!\")\n",
    "print(\"‚úÖ All data saved to unified mount directory\")\n",
    "print(\"üîÑ Results should now be visible across all environments\")\n",
    "print(f\"üåê S3 sync: Files saved to {{MOUNT_DIR}} should sync to S3 bucket\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Analysis workflow test completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(analysis_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"‚úÖ Analysis workflow completed\")\n",
    "        print(\"üìä Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if analysis was successful\n",
    "        analysis_success = \"üéâ Data analysis workflow completed successfully!\" in decoded_output\n",
    "        \n",
    "        return analysis_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data analysis workflow test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "analysis_success = await test_data_analysis_workflow()\n",
    "print(f\"\\nüéØ Analysis workflow result: {'‚úÖ PASSED' if analysis_success else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Test 4: Real Crypto Agent + E2B Analysis Workflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 01:29:19.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_cache_system\u001b[0m:\u001b[36m270\u001b[0m - \u001b[34m\u001b[1mInitialized generic cache system with TTL: 3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:19.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[34m\u001b[1mInitialized DataHTTPClient with 30.0s timeout\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:19.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_standard_configuration\u001b[0m:\u001b[36m559\u001b[0m - \u001b[34m\u001b[1mInitialized standard configuration: timeout=30.0s, retries=3, cache_ttl=3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:19.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing S3 mounted directory for binance: /opt/sentient\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ CRYPTO AGENT + E2B ANALYSIS WORKFLOW\n",
      "==================================================\n",
      "üß™ Project ID: agent_test_20250822_012919\n",
      "üìÅ Mount Directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "‚úÖ Using confirmed working mount: /opt/sentient\n",
      "\n",
      "üìä Step 1: Initializing real crypto data toolkits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 01:29:20.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_detect_e2b_context\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mS3 integration detected with bucket: roma-shared\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mData helpers initialized - Project: agent_test_20250822_012919, Toolkit: binance, Dir: /opt/sentient/agent_test_20250822_012919/binance, S3: True\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m300\u001b[0m - \u001b[34m\u001b[1mInitialized Multi-Market BinanceToolkit with default market 'spot' and 2 symbols\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m319\u001b[0m - \u001b[34m\u001b[1mUsing CoinGecko Pro API with API key\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_cache_system\u001b[0m:\u001b[36m270\u001b[0m - \u001b[34m\u001b[1mInitialized generic cache system with TTL: 3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[34m\u001b[1mInitialized DataHTTPClient with 30.0s timeout\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.440\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_standard_configuration\u001b[0m:\u001b[36m559\u001b[0m - \u001b[34m\u001b[1mInitialized standard configuration: timeout=30.0s, retries=3, cache_ttl=3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing S3 mounted directory for coingecko: /opt/sentient\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_detect_e2b_context\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mS3 integration detected with bucket: roma-shared\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mData helpers initialized - Project: agent_test_20250822_012919, Toolkit: coingecko, Dir: /opt/sentient/agent_test_20250822_012919/coingecko, S3: True\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m370\u001b[0m - \u001b[34m\u001b[1mInitialized CoinGeckoToolkit with default currency 'usd' (type: <enum 'VsCurrency'>) and all coins\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'spot' with base URL: https://api.binance.us\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'usdm' with base URL: https://fapi.binance.com\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'coinm' with base URL: https://dapi.binance.com\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36msetup_endpoints\u001b[0m:\u001b[36m419\u001b[0m - \u001b[34m\u001b[1mSetup 3 authenticated endpoints\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_get_client\u001b[0m:\u001b[36m175\u001b[0m - \u001b[34m\u001b[1mCreated HTTP client for endpoint 'spot'\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/exchangeInfo (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Real crypto toolkits initialized\n",
      "   BinanceToolkit: BinanceToolkit\n",
      "   CoinGeckoToolkit: CoinGeckoToolkit\n",
      "   Configured S3_MOUNT_DIR: /opt/sentient\n",
      "\n",
      "üí∞ Step 2: Fetching real crypto data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 01:29:21.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_cache_data\u001b[0m:\u001b[36m311\u001b[0m - \u001b[34m\u001b[1mCached set data (246 items) for key 'symbols_spot'\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit\u001b[0m:\u001b[36mreload_symbols\u001b[0m:\u001b[36m509\u001b[0m - \u001b[1mLoaded 246 symbols for Binance Spot Trading\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.570\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/klines (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Binance data fetched:\n",
      "   BTC Price: $N/A\n",
      "   ETH Price: $N/A\n",
      "   BTC 24h data points: 24\n",
      "‚ö†Ô∏è  CoinGecko data fetch error: 'CoinGeckoToolkit' object has no attribute 'get_coin_data'\n",
      "\n",
      "üöÄ Step 3: Initializing E2B for advanced analysis...\n",
      "‚úÖ Crypto analyzer agent with E2B initialized\n",
      "\n",
      "üîç Step 4: Running comprehensive crypto analysis...\n",
      "ü§ñ Agent query (preview): \n",
      "Perform a comprehensive cryptocurrency market analysis using the E2B sandbox environment.\n",
      "\n",
      "PROJECT SETUP:\n",
      "- Project ID: agent_test_20250822_012919\n",
      "- Mount directory: Use S3_MOUNT_DIR environment vari...\n",
      "\n",
      "‚è≥ Running analysis (this may take 2-3 minutes)...\n",
      "‚è∞ Analysis timed out after 5 minutes\n",
      "\n",
      "üîç Step 5: Verifying results in mount directory...\n",
      "‚úÖ Verification completed\n",
      "üìä Verification Output:\n",
      "----------------------------------------\n",
      "üîç RESULTS VERIFICATION\n",
      "==============================\n",
      "Project: agent_test_20250822_012919\n",
      "Expected mount: /opt/sentient\n",
      "S3_MOUNT_DIR env var: /opt/sentient\n",
      "\"√¢¬ú¬Ö Project directory exists: /opt/sentient/agent_test_20250822_012919\n",
      "√¢¬ú¬Ö Results directory exists: /opt/sentient/agent_test_20250822_012919/results\n",
      "\n",
      "√∞¬ü¬ì¬ã Results files (3):\n",
      "   √∞¬ü¬ì¬Ñ btc_advanced_analysis.png: 726,382 bytes\n",
      "   √∞¬ü¬ì¬Ñ btc_comprehensive_dashboard.png: 1,146,585 bytes\n",
      "   √∞¬ü¬ì¬Ñ crypto_price_comparison.png: 98,849 bytes\n",
      "\n",
      "√∞¬ü¬ì¬ä Total results size: 1,971,816 bytes\n",
      "√¢¬ù¬å Missing expected file: crypto_analysis_report.json\n",
      "√¢¬ù¬å Missing expected file: processed_data.csv\n",
      "√¢¬ù¬å Missing expected file: analysis_summary.md\n",
      "√¢¬ú¬Ö Found 3 visualization(s): ['btc_advanced_analysis.png'btc_comprehensive_dashboard.png'crypto_price_comparison.png']\n",
      "\n",
      "√∞¬ü¬ì¬à Success rate: 0/3 expected files found\n",
      "√∞¬ü¬ì¬Å Final mount directory used: /opt/sentient\n",
      "\n",
      "√¢¬ú¬Ö Verification completed\n",
      "\"\n",
      "----------------------------------------\n",
      "\n",
      "üéØ Crypto Agent + E2B Workflow Result: ‚ùå FAILED\n",
      "\n",
      "üéØ Final result: ‚ùå FAILED\n"
     ]
    }
   ],
   "source": [
    "async def test_crypto_agent_e2b_workflow():\n",
    "    \"\"\"Test real crypto agent data fetching followed by E2B analysis.\"\"\"\n",
    "    print(\"ü§ñ CRYPTO AGENT + E2B ANALYSIS WORKFLOW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "        \n",
    "        project_id = f\"agent_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"üß™ Project ID: {project_id}\")\n",
    "        print(f\"üìÅ Mount Directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Use discovered working mount if available from previous tests\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            working_mount = globals()['discovered_mount_dir']\n",
    "            print(f\"üîÑ Using discovered working mount: {working_mount}\")\n",
    "        elif 'working_mount_dir' in globals():\n",
    "            working_mount = globals()['working_mount_dir']\n",
    "            print(f\"‚úÖ Using confirmed working mount: {working_mount}\")\n",
    "        else:\n",
    "            working_mount = mount_dir\n",
    "            print(f\"üéØ Using configured mount: {working_mount}\")\n",
    "        \n",
    "        # Step 1: Initialize crypto data toolkits (real data fetching)\n",
    "        print(\"\\nüìä Step 1: Initializing real crypto data toolkits...\")\n",
    "        \n",
    "        try:\n",
    "            from sentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit import BinanceToolkit\n",
    "            from sentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit import CoinGeckoToolkit\n",
    "            \n",
    "            # Initialize with mount directory for data persistence\n",
    "            os.environ[\"CURRENT_PROJECT_ID\"] = project_id\n",
    "            os.environ[\"S3_MOUNT_ENABLED\"] = \"true\"\n",
    "            os.environ[\"S3_MOUNT_DIR\"] = working_mount  # Use working mount\n",
    "            \n",
    "            # Initialize toolkits - they should use BaseDataToolkit and save to mount\n",
    "            binance_toolkit = BinanceToolkit(\n",
    "                symbols=['BTCUSDT', 'ETHUSD'],\n",
    "                default_market_type=\"spot\"\n",
    "            )\n",
    "            \n",
    "            coingecko_toolkit = CoinGeckoToolkit()\n",
    "            \n",
    "            print(\"‚úÖ Real crypto toolkits initialized\")\n",
    "            print(f\"   BinanceToolkit: {binance_toolkit.__class__.__name__}\")\n",
    "            print(f\"   CoinGeckoToolkit: {coingecko_toolkit.__class__.__name__}\")\n",
    "            print(f\"   Configured S3_MOUNT_DIR: {working_mount}\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not import toolkits: {e}\")\n",
    "            print(\"   Using simulated data instead\")\n",
    "            binance_toolkit = None\n",
    "            coingecko_toolkit = None\n",
    "        \n",
    "        # Step 2: Fetch real crypto data using toolkits\n",
    "        print(\"\\nüí∞ Step 2: Fetching real crypto data...\")\n",
    "        \n",
    "        fetched_data = {}\n",
    "        \n",
    "        if binance_toolkit:\n",
    "            try:\n",
    "                # Fetch real current prices\n",
    "                btc_price = await binance_toolkit.get_current_price('BTCUSDT')\n",
    "                eth_price = await binance_toolkit.get_current_price('ETHUSD')\n",
    "                \n",
    "                # Fetch some historical data\n",
    "                btc_klines = await binance_toolkit.get_klines('BTCUSDT', interval='1h', limit=24)\n",
    "                \n",
    "                fetched_data['binance'] = {\n",
    "                    'btc_price': btc_price,\n",
    "                    'eth_price': eth_price,\n",
    "                    'btc_24h_data': btc_klines\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ Binance data fetched:\")\n",
    "                print(f\"   BTC Price: ${btc_price.get('price', 'N/A')}\")\n",
    "                print(f\"   ETH Price: ${eth_price.get('price', 'N/A')}\")\n",
    "                print(f\"   BTC 24h data points: {len(btc_klines.get('data', [])) if isinstance(btc_klines, dict) else 'N/A'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Binance data fetch error: {e}\")\n",
    "                fetched_data['binance_error'] = str(e)\n",
    "        \n",
    "        if coingecko_toolkit:\n",
    "            try:\n",
    "                # Get market data from CoinGecko\n",
    "                btc_market = await coingecko_toolkit.get_coin_data('bitcoin')\n",
    "                market_overview = await coingecko_toolkit.get_global_market_data()\n",
    "                \n",
    "                fetched_data['coingecko'] = {\n",
    "                    'btc_market': btc_market,\n",
    "                    'global_market': market_overview\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ CoinGecko data fetched:\")\n",
    "                print(f\"   Bitcoin market data: Available\")\n",
    "                print(f\"   Global market data: Available\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  CoinGecko data fetch error: {e}\")\n",
    "                fetched_data['coingecko_error'] = str(e)\n",
    "        \n",
    "        # Step 3: Initialize E2B for advanced analysis\n",
    "        print(\"\\nüöÄ Step 3: Initializing E2B for advanced analysis...\")\n",
    "        \n",
    "        from agno.tools.e2b import E2BTools\n",
    "        from agno.tools.reasoning import ReasoningTools\n",
    "        from agno.agent import Agent as AgnoAgent\n",
    "        from agno.models.litellm import LiteLLM\n",
    "        \n",
    "        # Create E2B-enabled crypto analyzer agent\n",
    "        crypto_analyzer_agent = AgnoAgent(\n",
    "            model=LiteLLM(id=\"openrouter/anthropic/claude-sonnet-4\"),\n",
    "            tools=[\n",
    "                E2BTools(sandbox_options={\"template\": template_name}),\n",
    "                ReasoningTools()\n",
    "            ],\n",
    "            name=\"CryptoAnalyzerWithE2B\",\n",
    "            system_message=f\"\"\"\n",
    "You are an expert cryptocurrency analyst with access to a secure E2B sandbox environment for advanced data analysis.\n",
    "\n",
    "Key capabilities:\n",
    "1. E2B sandbox for secure Python code execution\n",
    "2. Access to unified mount directory for data persistence\n",
    "3. Advanced data analysis libraries: pandas, numpy, matplotlib, seaborn\n",
    "4. Cross-environment data sharing\n",
    "\n",
    "IMPORTANT MOUNT DIRECTORY CONFIGURATION:\n",
    "- The mount directory is configured via S3_MOUNT_DIR environment variable\n",
    "- In E2B sandbox, check: os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "- Use this path consistently for all file operations\n",
    "- If mount directory doesn't exist, check alternative paths like /workspace/data\n",
    "\n",
    "When analyzing data:\n",
    "1. Get mount directory: MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "2. Save all analysis results to MOUNT_DIR/{{project_id}}/results/\n",
    "3. Use the E2B sandbox for all computations and visualizations\n",
    "4. Create comprehensive reports with actionable insights\n",
    "5. Ensure data persists across environments\n",
    "\n",
    "Always provide thorough analysis with clear explanations and save results for future access.\n",
    "\"\"\",\n",
    "            show_tool_calls=True,\n",
    "            markdown=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Crypto analyzer agent with E2B initialized\")\n",
    "        \n",
    "        # Step 4: Run comprehensive analysis using both real data and E2B\n",
    "        print(\"\\nüîç Step 4: Running comprehensive crypto analysis...\")\n",
    "        \n",
    "        analysis_query = f\"\"\"\n",
    "Perform a comprehensive cryptocurrency market analysis using the E2B sandbox environment.\n",
    "\n",
    "PROJECT SETUP:\n",
    "- Project ID: {project_id}\n",
    "- Mount directory: Use S3_MOUNT_DIR environment variable (fallback: /sentient)\n",
    "- Check available paths: /workspace/data, /home/user/data if primary mount fails\n",
    "\n",
    "ANALYSIS TASKS:\n",
    "1. Get mount directory from environment: MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "2. If MOUNT_DIR doesn't exist, try alternatives: /workspace/data, /home/user/data\n",
    "3. Create project directory structure in MOUNT_DIR/{project_id}/\n",
    "4. Process and analyze this real market data: {json.dumps(fetched_data, indent=2, default=str)}\n",
    "5. Perform technical analysis on the price data\n",
    "6. Calculate key metrics: volatility, returns, moving averages\n",
    "7. Generate market insights and trading signals\n",
    "8. Create visualizations (save as PNG files)\n",
    "9. Save comprehensive analysis report as JSON\n",
    "10. Verify all files are saved to the mount directory\n",
    "\n",
    "Deliverables:\n",
    "- Save analysis report to: MOUNT_DIR/{project_id}/results/crypto_analysis_report.json\n",
    "- Save processed data to: MOUNT_DIR/{project_id}/results/processed_data.csv  \n",
    "- Save visualizations to: MOUNT_DIR/{project_id}/results/*.png\n",
    "- Create summary file: MOUNT_DIR/{project_id}/results/analysis_summary.md\n",
    "\n",
    "IMPORTANT: Always check if the mount directory exists first, and include debugging info about paths used.\n",
    "\n",
    "Use the E2B sandbox for all computations and file operations. Ensure data persists in the unified mount directory.\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"ü§ñ Agent query (preview): {analysis_query[:200]}...\")\n",
    "        print(\"\\n‚è≥ Running analysis (this may take 2-3 minutes)...\")\n",
    "        \n",
    "        try:\n",
    "            # Run the analysis with timeout\n",
    "            analysis_response = await asyncio.wait_for(\n",
    "                crypto_analyzer_agent.arun(analysis_query),\n",
    "                timeout=300  # 5 minute timeout\n",
    "            )\n",
    "            \n",
    "            print(\"‚úÖ Analysis completed successfully!\")\n",
    "            print(\"\\nüìÑ Agent Response:\")\n",
    "            print(\"-\" * 60)\n",
    "            # Truncate very long responses for readability\n",
    "            if len(str(analysis_response)) > 2000:\n",
    "                print(f\"{str(analysis_response)[:2000]}...\\n[Response truncated - full response was {len(str(analysis_response))} characters]\")\n",
    "            else:\n",
    "                print(analysis_response)\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(\"‚è∞ Analysis timed out after 5 minutes\")\n",
    "            analysis_response = \"TIMEOUT\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Analysis failed: {e}\")\n",
    "            analysis_response = f\"ERROR: {e}\"\n",
    "        \n",
    "        # Step 5: Verify results in E2B mount directory\n",
    "        print(\"\\nüîç Step 5: Verifying results in mount directory...\")\n",
    "        \n",
    "        e2b_toolkit = E2BTools(sandbox_options={\"template\": template_name})\n",
    "        \n",
    "        verification_code = f\"\"\"\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Get mount directory from environment variable\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "PROJECT_ID = \"{project_id}\"\n",
    "\n",
    "print(\"üîç RESULTS VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Project: {{PROJECT_ID}}\")\n",
    "print(f\"Expected mount: {{MOUNT_DIR}}\")\n",
    "print(f\"S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}}\")\n",
    "\n",
    "# Check if mount directory exists, try alternatives if not\n",
    "if not os.path.exists(MOUNT_DIR):\n",
    "    print(f\"‚ùå Primary mount directory not found: {{MOUNT_DIR}}\")\n",
    "    \n",
    "    # Try alternative paths from startup.sh symlinks\n",
    "    alternatives = [\"/workspace/data\", \"/home/user/data\"]\n",
    "    for alt in alternatives:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"‚úÖ Using alternative mount: {{alt}}\")\n",
    "            MOUNT_DIR = alt\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚ùå No alternative mount directories found\")\n",
    "\n",
    "# Check project directory\n",
    "project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\n",
    "if os.path.exists(project_dir):\n",
    "    print(f\"‚úÖ Project directory exists: {{project_dir}}\")\n",
    "    \n",
    "    # Check results directory\n",
    "    results_dir = os.path.join(project_dir, \"results\")\n",
    "    if os.path.exists(results_dir):\n",
    "        print(f\"‚úÖ Results directory exists: {{results_dir}}\")\n",
    "        \n",
    "        # List all files in results\n",
    "        results_files = os.listdir(results_dir)\n",
    "        print(f\"\\\\nüìã Results files ({{len(results_files)}}):\")\n",
    "        \n",
    "        total_size = 0\n",
    "        for f in sorted(results_files):\n",
    "            f_path = os.path.join(results_dir, f)\n",
    "            if os.path.isfile(f_path):\n",
    "                size = os.path.getsize(f_path)\n",
    "                total_size += size\n",
    "                print(f\"   üìÑ {{f}}: {{size:,}} bytes\")\n",
    "                \n",
    "                # Show content preview for key files\n",
    "                if f.endswith('.json'):\n",
    "                    try:\n",
    "                        with open(f_path, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "                        print(f\"      üîç JSON keys: {{list(data.keys())[:5]}}\")\n",
    "                    except:\n",
    "                        pass\n",
    "            elif os.path.isdir(f_path):\n",
    "                sub_files = len(os.listdir(f_path))\n",
    "                print(f\"   üìÅ {{f}}: {{sub_files}} items\")\n",
    "        \n",
    "        print(f\"\\\\nüìä Total results size: {{total_size:,}} bytes\")\n",
    "        \n",
    "        # Check for expected files\n",
    "        expected_files = [\n",
    "            'crypto_analysis_report.json',\n",
    "            'processed_data.csv',\n",
    "            'analysis_summary.md'\n",
    "        ]\n",
    "        \n",
    "        found_files = 0\n",
    "        for expected in expected_files:\n",
    "            if expected in results_files:\n",
    "                print(f\"‚úÖ Found expected file: {{expected}}\")\n",
    "                found_files += 1\n",
    "            else:\n",
    "                print(f\"‚ùå Missing expected file: {{expected}}\")\n",
    "        \n",
    "        # Check for visualizations\n",
    "        png_files = [f for f in results_files if f.endswith('.png')]\n",
    "        if png_files:\n",
    "            print(f\"‚úÖ Found {{len(png_files)}} visualization(s): {{png_files}}\")\n",
    "        else:\n",
    "            print(\"‚ùå No PNG visualizations found\")\n",
    "        \n",
    "        print(f\"\\\\nüìà Success rate: {{found_files}}/{{len(expected_files)}} expected files found\")\n",
    "        print(f\"üìÅ Final mount directory used: {{MOUNT_DIR}}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Results directory not found: {{results_dir}}\")\n",
    "else:\n",
    "    print(f\"‚ùå Project directory not found: {{project_dir}}\")\n",
    "    print(f\"   Looking for: {{MOUNT_DIR}}/{{PROJECT_ID}}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Verification completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        verification_result = e2b_toolkit.run_python_code(verification_code)\n",
    "        decoded_verification = decode_e2b_output(verification_result)\n",
    "        print(\"‚úÖ Verification completed\")\n",
    "        print(\"üìä Verification Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_verification)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Success determination\n",
    "        success = (\n",
    "            analysis_response != \"TIMEOUT\" and\n",
    "            \"ERROR\" not in str(analysis_response) and\n",
    "            \"‚úÖ\" in str(decoded_verification)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéØ Crypto Agent + E2B Workflow Result: {'‚úÖ PASSED' if success else '‚ùå FAILED'}\")\n",
    "        \n",
    "        if success:\n",
    "            print(\"üéâ Complete workflow successful!\")\n",
    "            print(f\"   üìÅ Real data fetched from crypto APIs\")\n",
    "            print(f\"   ü§ñ Agent analysis completed in E2B sandbox\")\n",
    "            print(f\"   üíæ Results saved to unified mount directory\")\n",
    "            print(f\"   üîÑ Data accessible across all environments\")\n",
    "            print(f\"   üåê Using environment-variable mount: {working_mount}\")\n",
    "        \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Crypto agent + E2B workflow failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "crypto_agent_success = await test_crypto_agent_e2b_workflow()\n",
    "print(f\"\\nüéØ Final result: {'‚úÖ PASSED' if crypto_agent_success else '‚ùå FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_summary():\n",
    "    \"\"\"Print a comprehensive summary of all tests.\"\"\"\n",
    "    print(\"üìä E2B UNIFIED MOUNT TEST SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get mount directory from environment variable\n",
    "    configured_mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/data/sentient\")\n",
    "    configured_mount_dir = os.path.expandvars(configured_mount_dir)\n",
    "    \n",
    "    print(f\"üéØ Configured Mount Directory (S3_MOUNT_DIR): {configured_mount_dir}\")\n",
    "    \n",
    "    # Show discovered working mount if different\n",
    "    if 'discovered_mount_dir' in globals():\n",
    "        print(f\"üîÑ Discovered Working Mount: {globals()['discovered_mount_dir']}\")\n",
    "    if 'working_mount_dir' in globals():\n",
    "        print(f\"‚úÖ Confirmed Working Mount: {globals()['working_mount_dir']}\")\n",
    "    \n",
    "    # Collect test results\n",
    "    tests = [\n",
    "        (\"Mount Directory Verification\", globals().get('mount_success', False)),\n",
    "        (\"Project Structure Creation\", globals().get('structure_success', False)),\n",
    "        (\"Data Analysis Workflow\", globals().get('analysis_success', False)),\n",
    "        (\"Crypto Agent + E2B Analysis\", globals().get('crypto_agent_success', False)),\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"\\nüß™ Test Results:\")\n",
    "    for test_name, result in tests:\n",
    "        if result is None:\n",
    "            status = \"‚è≠Ô∏è  SKIPPED\"\n",
    "        elif result:\n",
    "            status = \"‚úÖ PASSED\"\n",
    "            passed += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            status = \"‚ùå FAILED\"\n",
    "            total += 1\n",
    "        \n",
    "        print(f\"  {status} {test_name}\")\n",
    "    \n",
    "    print(f\"\\nüìà Overall Results: {passed}/{total} tests passed\")\n",
    "    \n",
    "    # Results analysis\n",
    "    if passed == total and total > 0:\n",
    "        print(\"\\nüéâ SUCCESS: All tests passed!\")\n",
    "        print(\"‚úÖ Unified mount directory system is working correctly\")\n",
    "        print(\"‚úÖ Environment variable configuration (S3_MOUNT_DIR) is functional\")\n",
    "        print(\"‚úÖ Cross-environment data persistence is operational\")\n",
    "        print(\"‚úÖ Project structure creation and data workflows are working\")\n",
    "        print(\"‚úÖ Real crypto agent + E2B analysis integration is functional\")\n",
    "    elif passed > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  PARTIAL SUCCESS: {passed}/{total} tests passed\")\n",
    "        print(\"   Check the failed tests above for specific issues\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå FAILURE: No tests passed\")\n",
    "        print(\"   Check E2B template configuration and mount setup\")\n",
    "    \n",
    "    # Key information\n",
    "    print(\"\\nüí° Key System Information:\")\n",
    "    print(f\"  üìÅ Mount Directory: {configured_mount_dir} (from S3_MOUNT_DIR env var)\")\n",
    "    print(f\"  üèóÔ∏è  E2B Template: {os.getenv('E2B_TEMPLATE_ID', 'sentient-e2b-s3')}\")\n",
    "    print(f\"  üóÇÔ∏è  Project Structure: MOUNT_DIR/{{project_id}}/{{toolkit_name|results}}/\")\n",
    "    print(f\"  üîÑ Data Flow: Local ‚Üí S3 ‚Üí E2B sandbox (consistent paths)\")\n",
    "    print(f\"  ü§ñ Agent Integration: Real API data ‚Üí E2B analysis ‚Üí Unified results\")\n",
    "    print(f\"  üåê Environment Variable: All components use S3_MOUNT_DIR for consistency\")\n",
    "    \n",
    "    # Usage guidance\n",
    "    print(\"\\nüîó Usage Instructions:\")\n",
    "    print(\"  1. Set project ID: export CURRENT_PROJECT_ID='your_project'\")\n",
    "    print(\"  2. Enable S3 mount: export S3_MOUNT_ENABLED='true'\")\n",
    "    print(f\"  3. Configure mount path: export S3_MOUNT_DIR='{configured_mount_dir}'\")\n",
    "    print(\"  4. BaseDataToolkit will auto-create: $S3_MOUNT_DIR/your_project/{toolkit_name}/\")\n",
    "    print(\"  5. AI analysis results go to: $S3_MOUNT_DIR/your_project/results/\")\n",
    "    print(\"  6. All data syncs automatically across environments\")\n",
    "    print(\"  7. Environment variable ensures consistency across all components\")\n",
    "    \n",
    "    # Troubleshooting\n",
    "    if passed < total:\n",
    "        print(\"\\nüîß Troubleshooting:\")\n",
    "        if not globals().get('mount_success', False):\n",
    "            print(\"  üìÅ Mount issues: E2B template lacks proper S3 mount or AWS credentials\")\n",
    "            print(\"    ‚ó¶ Root cause: Template built without AWS build arguments\")\n",
    "            print(\"    ‚ó¶ Solution: Rebuild template with: ./setup.sh --e2b-only\")\n",
    "        if not globals().get('structure_success', False):\n",
    "            print(\"  üóÇÔ∏è  Structure issues: Check write permissions to mount directory\")\n",
    "        if not globals().get('analysis_success', False):\n",
    "            print(\"  üìä Analysis issues: Verify pandas/numpy are available in E2B template\")\n",
    "        if not globals().get('crypto_agent_success', False):\n",
    "            print(\"  ü§ñ Agent issues: Check crypto toolkit imports and E2B agent configuration\")\n",
    "        \n",
    "        print(\"\\n  üõ†Ô∏è  Root Cause & Fix:\")\n",
    "        print(\"    ‚ùå PROBLEM: E2B template was built without AWS credentials as build arguments\")\n",
    "        print(\"    ‚úÖ SOLUTION: Rebuild E2B template with proper AWS credentials:\")\n",
    "        print(\"       cd docker/e2b-sandbox\")\n",
    "        print(\"       e2b template build \\\\\")\n",
    "        print(\"         --build-arg AWS_ACCESS_KEY_ID='your_key' \\\\\")\n",
    "        print(\"         --build-arg AWS_SECRET_ACCESS_KEY='your_secret' \\\\\")\n",
    "        print(\"         --build-arg S3_BUCKET_NAME='your_bucket' \\\\\")\n",
    "        print(f\"         --build-arg S3_MOUNT_DIR='{configured_mount_dir}' \\\\\")\n",
    "        print(\"         --name sentient-e2b-s3 --force\")\n",
    "        print()\n",
    "        print(\"    Or run automated fix: ./setup.sh --e2b-only\")\n",
    "        \n",
    "        # Show temporary workaround if available\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            print(f\"\\n  üîß TEMPORARY WORKAROUND:\")\n",
    "            print(f\"    Update .env with working mount: S3_MOUNT_DIR={globals()['discovered_mount_dir']}\")\n",
    "\n",
    "print_test_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "### ‚úÖ If all tests passed:\n",
    "Your unified mount directory system is working perfectly! You can now:\n",
    "\n",
    "1. **Use in production**: Data saved in any environment will be visible in all others\n",
    "2. **Run AI agents**: They will automatically use the correct project structure\n",
    "3. **Scale across environments**: Local development ‚Üí Docker deployment ‚Üí E2B execution\n",
    "\n",
    "### ‚ö†Ô∏è If some tests failed:\n",
    "1. **Check the error messages** in the test outputs above\n",
    "2. **Verify E2B template**: Ensure `/data/sentient` is properly mounted\n",
    "3. **Check S3 setup**: Verify AWS credentials and bucket permissions\n",
    "4. **Rebuild template**: `cd docker/e2b-sandbox && e2b template build -n sentient-e2b-s3`\n",
    "\n",
    "### üéØ Production Usage:\n",
    "\n",
    "```bash\n",
    "# Set your project context\n",
    "export CURRENT_PROJECT_ID=\"crypto_analysis_2024\"\n",
    "export S3_MOUNT_ENABLED=\"true\"\n",
    "\n",
    "# Run your agents - they'll automatically create:\n",
    "# /data/sentient/crypto_analysis_2024/binance_toolkit/\n",
    "# /data/sentient/crypto_analysis_2024/coingecko_toolkit/\n",
    "# /data/sentient/crypto_analysis_2024/results/\n",
    "```\n",
    "\n",
    "### üìÅ Expected Final Structure:\n",
    "```\n",
    "/data/sentient/\n",
    "‚îú‚îÄ‚îÄ crypto_analysis_2024/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ binance_toolkit/          # Real API data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BTCUSDT_data.parquet\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ current_prices.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ coingecko_toolkit/        # Market data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ results/                  # AI analysis outputs\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ market_analysis.json\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trading_signals.csv\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ risk_assessment.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ other_projects/\n",
    "```\n",
    "\n",
    "### üéâ Benefits Achieved:\n",
    "- ‚úÖ **Unified file paths** across all environments\n",
    "- ‚úÖ **Automatic data sync** via S3 mount\n",
    "- ‚úÖ **Project isolation** with clear directory structure\n",
    "- ‚úÖ **Cross-environment persistence** for AI workflows\n",
    "- ‚úÖ **Scalable architecture** for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
