project: roma-dspy
version: "0.1.0"
environment: development

agents:
  atomizer:
    llm:
      model: "openrouter/google/gemini-2.5-flash-lite-preview-09-2025"
      temperature: 0.7
      max_tokens: 128000
      timeout: 30
      api_key: ${oc.env:OPENROUTER_API_KEY,null}
      base_url: https://openrouter.ai/api/v1
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

  planner:
    llm:
      model: "openrouter/google/gemini-2.5-flash-preview-09-2025"
      temperature: 0.4
      max_tokens: 128000
      timeout: 30
      api_key: ${oc.env:OPENROUTER_API_KEY,null}
      base_url: https://openrouter.ai/api/v1
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config:
      max_subtasks: 15
    strategy_config: {}

  executor:
    llm:
      model: "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905"
      temperature: 0.75
      max_tokens: 128000
      timeout: 30
      api_key: ${oc.env:FIREWORKS_API_KEY,null}
      base_url: https://api.fireworks.ai/inference/v1
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

  aggregator:
    llm:
      model: "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905"
      temperature: 0.75
      max_tokens: 128000
      timeout: 30
      api_key: ${oc.env:FIREWORKS_API_KEY,null}
      base_url: https://api.fireworks.ai/inference/v1
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

  verifier:
    llm:
      model: "gpt-4o-mini"
      temperature: 0.1
      max_tokens: 2000
      timeout: 30
      api_key: ${oc.env:OPENROUTER_API_KEY,null}
      base_url: https://openrouter.ai/api/v1
      num_retries: 3
      cache: true
      rollout_id: null
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: false
    type: null
    task_type: null
    signature: null
    signature_instructions: null
    agent_config: {}
    strategy_config: {}

agent_mapping:
  executors:
    RETRIEVE:
      llm:
        model: "openrouter/openai/gpt-5-mini:online"
        temperature: 1.0
        max_tokens: 128000
        timeout: 500
        num_retries: 3
        cache: true
        api_key: ${oc.env:OPENROUTER_API_KEY,null}
        base_url: https://openrouter.ai/api/v1
      prediction_strategy: "chain_of_thought"
      toolkits: []
      enabled: true
      agent_config: {}
      strategy_config: {}

    THINK:
      llm:
        model: "cerebras/gpt-oss-120b"
        temperature: 0.7
        max_tokens: 128000
        timeout: 30
        num_retries: 3
        cache: true
      prediction_strategy: "chain_of_thought"
      toolkits: []
      enabled: true
      agent_config: {}
      strategy_config: {}

    WRITE:
      llm:
        model: "cerebras/gpt-oss-120b"
        temperature: 0.7
        max_tokens: 128000
        timeout: 30
        num_retries: 3
        cache: true
      prediction_strategy: "chain_of_thought"
      toolkits: []
      enabled: true
      agent_config: {}
      strategy_config: {}

  default_executor:
    llm:
      model: "fireworks_ai/accounts/fireworks/models/gpt-oss-120b"
      temperature: 0.7
      max_tokens: 128000
      timeout: 30
      num_retries: 3
      cache: true
      api_key: ${oc.env:FIREWORKS_API_KEY,null}
      base_url: https://api.fireworks.ai/inference/v1
    prediction_strategy: "chain_of_thought"
    toolkits: []
    enabled: true
    agent_config: {}
    strategy_config: {}

runtime:
  max_concurrency: 8
  timeout: 30
  verbose: false
  max_depth: 1
  enable_logging: false
  log_level: DEBUG

  cache:
    enabled: true
    enable_disk_cache: true
    enable_memory_cache: true
    disk_cache_dir: .cache/dspy
    disk_size_limit_bytes: 30000000000
    memory_max_entries: 1000000

resilience:
  retry_strategy: "exponential_backoff"
  max_retries: 3
  base_delay: 1.0
  max_delay: 60.0
  jitter_factor: 0.1
  failure_threshold: 5
  recovery_timeout: 60.0
  success_threshold: 2
  evaluation_window: 300.0

  checkpoint:
    enabled: false
    storage_path: ${oc.env:STORAGE_BASE_PATH,/opt/sentient}/.checkpoints
    auto_checkpoint_triggers:
      - before_planning
      - before_aggregation
    max_checkpoints: 10
    max_age_hours: 24.0
    cleanup_interval_minutes: 60
    default_recovery_strategy: partial
    preserve_partial_results: true
    compress_checkpoints: true
    verify_integrity: true
    periodic_checkpoints_enabled: false
    periodic_interval_seconds: 30.0
    min_execution_time_for_periodic: 10.0

storage:
  base_path: ${oc.env:STORAGE_BASE_PATH,/opt/sentient}/.tmp/sentient
  max_file_size: 104857600
  buffer_size: 1048576
  postgres:
    enabled: false
    connection_url: postgresql+asyncpg://localhost/roma_dspy
    pool_size: 5
    max_overflow: 10
    pool_timeout: 30.0
    echo_sql: false

observability:
  mlflow:
    enabled: false
    tracking_uri: http://127.0.0.1:5000
    experiment_name: ROMA-DSPy
    log_traces: true
    log_traces_from_compile: false
    log_traces_from_eval: true
    log_compiles: true
    log_evals: true
    backend_store_uri: null
    artifact_location: null
  toolkit_metrics:
    enabled: true
    track_lifecycle: true
    track_invocations: true
    sample_rate: 1.0
    persist_to_db: true
    persist_to_mlflow: false
    batch_size: 100
    async_persist: true
  event_traces:
    enabled: true
    track_execution_events: true
    track_module_events: true
    track_task_lifecycle: true
    track_failures: true
    sample_rate: 1.0
    persist_to_db: true
    persist_to_mlflow: false
    batch_size: 50
    async_persist: true
    include_task_details: true
    include_timing: true
    max_goal_length: 200

logging:
  level: ERROR
  log_dir: logs/test/
  console_format: default
  file_format: detailed
  colorize: true
  serialize: false
  rotation: "100 MB"
  retention: "30 days"
  compression: "zip"
  intercept_standard_logging: true
  backtrace: true
  diagnose: false
  enqueue: true
